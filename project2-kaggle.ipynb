{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":66638,"databundleVersionId":7378729,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## <font style=\"color:green\">1. Data Loader [10 Points]</font>\n\nIn this section, you have to write a class or methods, which will be used to get training and validation data loader.\n\nYou need to write a custom dataset class to load data.\n\n**Note; There is   no separate validation data. , You will thus have to create your own validation set, by dividing the train data into train and validation data. Usually, we do 80:20 ratio for train and validation, respectively.**\n\n\nFor example:\n\n```python\nclass KenyanFood13Dataset(Dataset):\n    \"\"\"\n    \n    \"\"\"\n    \n    def __init__(self, *args):\n    ....\n    ...\n    \n    def __getitem__(self, idx):\n    ...\n    ...\n    \n\n```\n\n\n```python\ndef get_data(args1, *args):\n    ....\n    ....\n    return train_loader, test_loader\n```","metadata":{}},{"cell_type":"markdown","source":"# <font style=\"color:blue\">Project 2: Kaggle Competition - Classification</font>\n\n#### Maximum Points: 100\n\n<div>\n    <table>\n        <tr><td><h3>Sr. no.</h3></td> <td><h3>Section</h3></td> <td><h3>Points</h3></td> </tr>\n        <tr><td><h3>1</h3></td> <td><h3>Data Loader</h3></td> <td><h3>10</h3></td> </tr>\n        <tr><td><h3>2</h3></td> <td><h3>Configuration</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>3</h3></td> <td><h3>Evaluation Metric</h3></td> <td><h3>10</h3></td> </tr>\n        <tr><td><h3>4</h3></td> <td><h3>Train and Validation</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>5</h3></td> <td><h3>Model</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>6</h3></td> <td><h3>Utils</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>7</h3></td> <td><h3>Experiment</h3></td><td><h3>5</h3></td> </tr>\n        <tr><td><h3>8</h3></td> <td><h3>TensorBoard Dev Scalars Log Link</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>9</h3></td> <td><h3>Kaggle Profile Link</h3></td> <td><h3>50</h3></td> </tr>\n    </table>\n</div>\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport shutil\nimport os\nimport sys\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms\nfrom PIL import Image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Load your dataset\ndata = pd.read_csv(r'/kaggle/input/opencv-pytorch-classification-project-2/train.csv')\n\n# Define the split ratio (e.g., 80% training and 20% validation)\ntrain_size = 0.8\n\n# Split the data\ntrain_data, validation_data = train_test_split(data, train_size=train_size, random_state=42)\n\n# Save the split datasets into new CSV files\ntrain_data.to_csv('train_data.csv', index=False)\nvalidation_data.to_csv('validation_data.csv', index=False)\n\nprint(len(train_data))\n\nprint(len(validation_data))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create new directory for training set\nif not os.path.exists('training'):\n    os.mkdir('training')\n\n# Read your train CSV file for lable\nlabels=pd.read_csv(r'/kaggle/working/train_data.csv')\n\n#DataFrame 'labels' with columns 'id' and 'class'\n\n\n# Set the base directory where your images are located\nbase_image_dir = '/kaggle/input/opencv-pytorch-classification-project-2/images/images/'\ndst_dir=r\"/kaggle/working/training\"\n# Iterate over each row in the labels DataFrame\nfor _, row in labels.iterrows():\n    image_id = row['id']\n    class_label = str(row['class'])\n\n    # Construct source and destination paths\n    src_path = os.path.join(base_image_dir, f'{image_id}.jpg')\n    dst_path = os.path.join(dst_dir, class_label, f'{image_id}.jpg')\n\n    try:\n        # Check if the source file exists\n        if os.path.exists(src_path):\n            # Create the destination directory if needed\n            os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n            shutil.copy(src_path, dst_path)\n            print(f\"Successfully copied {src_path} to {dst_path}\")\n        else:\n            print(f\"Source file {src_path} does not exist.\")\n    except Exception as e:\n        print(f\"Error copying {src_path} to {dst_path}: {e}\")\n\nprint(\"All images processed!\")\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create new directory for training set\nif not os.path.exists('validation'):\n    os.mkdir('validation')\n\nval_lables=pd.read_csv(r'/kaggle/working/validation_data.csv')\n#DataFrame 'labels' with columns 'id' and 'class'\n\n\n# Set the base directory where your images are located\nbase_image_dir = '/kaggle/input/opencv-pytorch-classification-project-2/images/images/'\ndst_dir=r\"/kaggle/working/validation\"\n# Iterate over each row in the labels DataFrame\nfor _, row in labels.iterrows():\n    image_id = row['id']\n    class_label = str(row['class'])\n\n    # Construct source and destination paths\n    src_path = os.path.join(base_image_dir, f'{image_id}.jpg')\n    dst_path = os.path.join(dst_dir, class_label, f'{image_id}.jpg')\n\n    try:\n        # Check if the source file exists\n        if os.path.exists(src_path):\n            # Create the destination directory if needed\n            os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n            shutil.copy(src_path, dst_path)\n            print(f\"Successfully copied {src_path} to {dst_path}\")\n        else:\n            print(f\"Source file {src_path} does not exist.\")\n    except Exception as e:\n        print(f\"Error copying {src_path} to {dst_path}: {e}\")\n\nprint(\"All images processed!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read your test CSV file for geting test image set\ntest_img=pd.read_csv(r'/kaggle/input/opencv-pytorch-classification-project-2/test.csv')\n\n#create new directory for test set\nif not os.path.exists('test'):\n    os.mkdir('test')\n# Set the base directory where your images are located\nbase_image_dir = '/kaggle/input/opencv-pytorch-classification-project-2/images/images/'\ntest_dst=r\"/kaggle/working/test\"#destination for test images\n\n# Iterate over each row in the labels DataFrame\nfor _, row in test_img.iterrows():\n    image_id = row['id']\n    \n# Construct source and destination paths\n    src_path = os.path.join(base_image_dir, f'{image_id}.jpg')\n    dst_path = os.path.join(test_dst, f'{image_id}.jpg')\n    \n    try:\n        # Check if the source file exists\n        if os.path.exists(src_path):\n            # Create the destination directory if needed\n            #os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n            shutil.copy(src_path, dst_path)\n            print(f\"Successfully copied {src_path} to {dst_path}\")\n        else:\n            print(f\"Source file {src_path} does not exist.\")\n    except Exception as e:\n        print(f\"Error copying {src_path} to {dst_path}: {e}\")\n\nprint(\"All images processed!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\nimport os\nimport time\n\nfrom typing import Iterable\nfrom dataclasses import dataclass\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\n\nfrom torchvision import datasets, transforms, models\n\nfrom torch.optim import lr_scheduler\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport pandas as pd\n\nfrom PIL import Image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class KenyanFood13Dataset(Dataset):\n    \"\"\"\n    Custom Dataset for Kenyan Food Images with 13 classes.\n    \n    \"\"\"\n    \n\n    def __init__(self, dataframe,data_root,is_train, transform=None):\n        \"\"\"\n        Initializes the dataset.\n\n        Parameters:\n        - data_root (str): Path to the dataset directory.\n        - transform (callable, optional): Optional transform to be applied on a sample.\n        \"\"\"\n        self.dataframe= dataframe\n        self.data_root = data_root\n        self.transform = transform\n        self.is_train= is_train\n          \n              \n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        img_path, label = self.dataframe.iloc[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n\n        if self.transform:\n            image = self.transform(image)\n        # return the image and its associated labels and path\n        target = self.labels[idx]\n        \n        return image, label\n\n\n            \n        \n        image=Image.fromarray(image1)\n        if self.is_train:\n            lable_key=self.dataframe.iloc[idx,1]\n            lable=torch.tensor(str(lable_key))\n        else:\n            lable=torch.tensor(1)\n        if self.transform :\n            image=self.transform(image)\n\n        return image, label\n    \n    def image_path(self, idx):\n        return self.data_path[idx]\n\n    def class_name(self, label):\n        return id_to_class[label]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def image_preprocess_transforms():\n\n    preprocess = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor()\n        ])\n\n    return preprocess","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def image_common_transforms(mean=(0.5773, 0.4627, 0.3468), std=(0.2387, 0.2470, 0.2473)):\n    preprocess = image_preprocess_transforms()\n\n    common_transforms = transforms.Compose([\n        preprocess,\n        transforms.Normalize(mean, std)\n    ])\n\n    return common_transforms","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_loader(data, transform, batch_size=32, shuffle=False, num_workers=2):\n\n    dataset =  KenyanFood13Dataset(data_root,is_train, transform=None)\n\n    loader = DataLoader(dataset,\n                        batch_size=batch_size,\n                        num_workers=num_workers,\n                        shuffle=shuffle)\n\n    return loader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_data(train_data, val_data, batch_size, num_workers=4, data_augmentation=False):\n\n\n    mean=(0.5773, 0.4627, 0.3468)\n    std=(0.2387, 0.2470, 0.2473)\n\n    common_transforms = image_common_transforms()\n\n\n    # if data_augmentation is true\n    # data augmentation implementation\n    if data_augmentation:\n        train_transforms = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ColorJitter(brightness = 0.6),\n            transforms.RandomChoice([\n                transforms.RandomHorizontalFlip(),\n                transforms.RandomVerticalFlip(),\n                transforms.RandomRotation(15, fill=(0,0,0)),\n                transforms.RandomCrop(224, padding=4),\n                transforms.RandomAffine(25, translate=(0.3,0.3),\n                                        scale=(0.8, 1.2), shear=None)\n            ]),\n\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std)\n        ])\n    # else do common transforms\n    else:\n        train_transforms = common_transforms\n\n\n    # train dataloader\n\n    train_loader = data_loader(train_data,\n                               train_transforms,\n                               batch_size=batch_size,\n                               shuffle=True,\n                               num_workers=num_workers)\n\n    # test dataloader\n\n\n    test_loader = data_loader(val_data,\n                              common_transforms,\n                              batch_size=batch_size,\n                              shuffle=False,\n                              num_workers=num_workers)\n\n    return train_loader, test_loader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"   \ntrain_dataroot=r\"/kaggle/input/opencv-pytorch-classification-project-2/images\"\ntrain_dataframe=pd.read_csv('/kaggle/working/train_data.csv')\ntrain_dataset = KenyanFood13Dataset(train_dataframe, train_dataroot,is_train=True, transform=None)\n\n \nval_dataroot =r\"/kaggle/input/opencv-pytorch-classification-project-2/images\" \nval_dataframe=pd.read_csv('/kaggle/working/validation_data.csv')\nval_dataset =KenyanFood13Dataset(val_dataframe,val_dataroot,is_train=False ,transform=None)  \ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n    \nprint(len(train_loader))\nprint(len(val_loader))\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@dataclass\nclass SystemConfiguration:\n    '''\n    Describes the common system setting needed for reproducible training\n    '''\n    seed: int = 21  # seed number to set the state of all random number generators\n    cudnn_benchmark_enabled: bool = True  # enable CuDNN benchmark for the sake of performance\n    cudnn_deterministic: bool = True  # make cudnn deterministic (reproducible training)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">2. Configuration [5 Points]</font>\n\n**Define your configuration here.**\n\nFor example:\n\n\n```python\n@dataclass\nclass TrainingConfiguration:\n    '''\n    Describes configuration of the training process\n    '''\n    batch_size: int = 10 \n    epochs_count: int = 50  \n    init_learning_rate: float = 0.1  # initial learning rate for lr scheduler\n    log_interval: int = 5  \n    test_interval: int = 1  \n    data_root: str = \"/kaggle/input/opencv-pytorch-classification-project-2/\" \n    num_workers: int = 2  \n    device: str = 'cuda'  \n    \n```","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass TrainingConfiguration:\n    '''\n    Describes configuration of the training process\n    '''\n    batch_size: int = 32\n    epochs_count: int = 60\n    init_learning_rate: float = 0.0001  # initial learning rate for lr scheduler\n    log_interval: int = 10\n    test_interval: int = 1\n    num_workers: int = 8\n    device: str = 'cuda'\n    decay_rate:float=0.2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def setup_system(system_config: SystemConfiguration) -> None:\n    torch.manual_seed(system_config.seed)\n    if torch.cuda.is_available():\n        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(\n    train_config: TrainingConfiguration, model: nn.Module, optimizer: torch.optim.Optimizer,\n    train_loader: torch.utils.data.DataLoader, epoch_idx: int\n) -> None:\n\n    # change model in training mood\n    model.train()\n\n    # to get batch loss\n    batch_loss = np.array([])\n\n    # to get batch accuracy\n    batch_acc = np.array([])\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n\n        # clone target\n        indx_target = target.clone()\n        # send data to device (its is medatory if GPU has to be used)\n        data = data.to(train_config.device)\n        # send target to device\n        target = target.to(train_config.device)\n\n        # reset parameters gradient to zero\n        optimizer.zero_grad()\n\n        # forward pass to the model\n        output = model(data)\n\n        # cross entropy loss\n        loss = F.cross_entropy(output, target)\n\n        # find gradients w.r.t training parameters\n        loss.backward()\n        # Update parameters using gardients\n        optimizer.step()\n\n        batch_loss = np.append(batch_loss, [loss.item()])\n\n        # Score to probability using softmax\n        prob = F.softmax(output, dim=1)\n\n        # get the index of the max probability\n        pred = prob.data.max(dim=1)[1]\n\n        # correct prediction\n        correct = pred.cpu().eq(indx_target).sum()\n\n        # accuracy\n        acc = float(correct) / float(len(data))\n\n        batch_acc = np.append(batch_acc, [acc])\n\n    epoch_loss = batch_loss.mean()\n    epoch_acc = batch_acc.mean()\n    print('Epoch: {} \\nTrain Loss: {:.6f} Acc: {:.4f}'.format(epoch_idx, epoch_loss, epoch_acc))\n    return epoch_loss, epoch_acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate(\n    train_config: TrainingConfiguration,\n    model: nn.Module,\n    test_loader: torch.utils.data.DataLoader,\n) -> float:\n    #\n    model.eval()\n    test_loss = 0\n    count_corect_predictions = 0\n    for data, target in val_loader:\n        \n        data = data.to(train_config.device)\n\n        target = target.to(train_config.device)\n\n        with torch.no_grad():\n            output = model(data)\n\n        # add loss for each mini batch\n        test_loss += F.cross_entropy(output, target).item()\n\n        # Score to probability using softmax\n        prob = F.softmax(output, dim=1)\n\n        # get the index of the max probability\n        pred = prob.data.max(dim=1)[1]\n\n        # add correct prediction count\n        count_corect_predictions += pred.cpu().eq(indx_target).sum()\n\n    # average over number of mini-batches\n    test_loss = test_loss / len(val_loader)\n\n    # average over number of dataset\n    accuracy = 100. * count_corect_predictions / len(val_loader.dataset)\n\n    print(\n        '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n            test_loss, count_corect_predictions, len(val_loader.dataset), accuracy\n        )\n    )\n\n    return test_loss, accuracy/100.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_model(model, device, model_dir='models', model_file_name='KenyanFood13.pt'):\n\n\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n\n    model_path = os.path.join(model_dir, model_file_name)\n\n    # make sure you transfer the model to cpu.\n    if device == 'cuda':\n        model.to('cpu')\n\n    # save the state_dict\n    torch.save(model.state_dict(), model_path)\n\n    if device == 'cuda':\n        model.to('cuda')\n\n    return","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_model(model, model_dir='models', model_file_name='KenyanFood13.pt'):\n    model_path = os.path.join(model_dir, model_file_name)\n\n    # loading the model and getting model parameters by using load_state_dict\n    model.load_state_dict(torch.load(model_path))\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main(model, optimizer, scheduler=None, system_configuration=SystemConfiguration(),\n         training_configuration=TrainingConfiguration()):\n\n    # system configuration\n    setup_system(system_configuration)\n\n    # batch size\n    batch_size_to_set = training_configuration.batch_size\n    # num_workers\n    num_workers_to_set = training_configuration.num_workers\n    # epochs\n    epoch_num_to_set = training_configuration.epochs_count\n\n    # if GPU is available use training config,\n    # else lowers batch_size, num_workers and epochs count\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    else:\n        device = \"cpu\"\n        batch_size_to_set = 32\n        num_workers_to_set = 4\n\n\n    \n\n    # Update training configuration\n    training_configuration = TrainingConfiguration(\n        device=device,\n        batch_size=batch_size_to_set,\n        num_workers=num_workers_to_set\n    )\n\n    # send model to device (GPU/CPU)\n    model.to(training_configuration.device)\n\n    best_loss = torch.tensor(np.inf)\n\n    # epoch train/test loss\n    epoch_train_loss = np.array([])\n    epoch_test_loss = np.array([])\n\n    # epch train/test accuracy\n    epoch_train_acc = np.array([])\n    epoch_test_acc = np.array([])\n\n    # Calculate Initial Test Loss\n    init_val_loss, init_val_accuracy = validate(training_configuration, model, val_loader)\n    print(\"Initial Test Loss : {:.6f}, \\nInitial Test Accuracy : {:.3f}%\\n\".format(init_val_loss,\n                                                                                   init_val_accuracy*100))\n\n    # trainig time measurement\n    t_begin = time.time()\n    for epoch in range(training_configuration.epochs_count):\n\n        # Train\n        train_loss, train_acc = train(training_configuration, model, optimizer, train_loader, epoch)\n\n        epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n\n        epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n\n        elapsed_time = time.time() - t_begin\n        speed_epoch = elapsed_time / (epoch + 1)\n        speed_batch = speed_epoch / len(train_loader)\n        eta = speed_epoch * training_configuration.epochs_count - elapsed_time\n\n        print(\n            \"Elapsed {:.2f}s, {:.2f} s/epoch, {:.2f} s/batch, ets {:.2f}s\".format(\n                elapsed_time, speed_epoch, speed_batch, eta\n            )\n        )\n\n        # Validate\n        if epoch % training_configuration.test_interval == 0:\n            current_loss, current_accuracy = validate(training_configuration, model, val_loader)\n\n            epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n\n            epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n\n            if current_loss < best_loss:\n                best_loss = current_loss\n                print('Model Improved. Saving the Model...\\n')\n                save_model(model, device=training_configuration.device)\n\n\n    print(\"Total time: {:.2f}, Best Loss: {:.3f}\".format(time.time() - t_begin, best_loss))\n\n    return model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_optimizer_and_scheduler(model):\n    train_config = TrainingConfiguration()\n\n    init_learning_rate = train_config.init_learning_rate\n\n\n    optimizer = optim.Adam(\n        model.parameters(),\n\n        lr =TrainingConfiguration.init_learning_rate,\n        \n    )\n\n    decay_rate = TrainingConfiguration.decay_rate\n\n    lmbda = lambda epoch: 1/(1 + decay_rate * epoch)\n\n    # Scheduler\n    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lmbda)\n\n    return optimizer, scheduler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">3. Evaluation Metric [10 Points]</font>\n\n**Define methods or classes that will be used in model evaluation. For example, accuracy, f1-score etc.**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_prediction(model, data, mean, std):\n\n    batch_size = 60\n\n    if torch.cuda.is_available():\n        device = \"cuda\"\n        num_workers = 8\n    else:\n        device = \"cpu\"\n        num_workers = 2\n\n\n\n    # transformed data\n    val_dataset_trans = Food13Dataset(data, transform=image_common_transforms())\n\n\n    data_len = val_dataset_trans.__len__()\n    total_batches = data_len//batch_size\n    rem_batch = data_len % batch_size\n\n    targets = []\n    classes = []\n\n    count = 0\n    # We prefer to do the predictions in batches and combine them later\n    for j in range(total_batches):\n\n        inputs = []\n        print(\"Getting predictions for batch \",j+1,\"...\")\n        for i in range(batch_size):\n            index = count\n            trans_input, target = val_dataset_trans.__getitem__(index)\n\n\n            inputs.append(trans_input)\n            targets.append(target)\n            count+= 1\n\n        inputs = torch.stack(inputs)\n\n\n\n        cls, prob = prediction(model, device, batch_input=inputs)\n        classes.extend([item for item in cls])\n\n\n    # Calculate predictions for the last remaining batch\n    inputs = []\n    print(\"Getting predictions for the last batch...\")\n    for i in range(rem_batch):\n        index = count\n        trans_input, target = val_dataset_trans.__getitem__(index)\n\n\n        inputs.append(trans_input)\n        targets.append(target)\n        count+= 1\n\n    inputs = torch.stack(inputs)\n\n    cls, prob = prediction(model, device, batch_input=inputs)\n    classes.extend([item for item in cls])\n\n    return np.array(classes), np.array(targets)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def accuracy_and_confusion_matrix(target, cls, id_to_class):\n\n    accuracy = accuracy_score(target, cls)\n    cn_matrix = confusion_matrix(target,cls, labels = list(id_to_class.keys()))\n\n    return accuracy, cn_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prediction(model, device, batch_input):\n\n    # send model to cpu/cuda according to your system configuration\n    model.to(device)\n\n    # it is important to do model.eval() before prediction\n    model.eval()\n\n    data = batch_input.to(device)\n\n    output = model(data)\n\n    # Score to probability using softmax\n    prob = F.softmax(output, dim=1)\n\n    # get the max probability\n    pred_prob = prob.data.max(dim=1)[0]\n\n    # get the index of the max probability\n    pred_index = prob.data.max(dim=1)[1]\n\n    return pred_index.cpu().numpy(), pred_prob.cpu().numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_loss_accuracy(train_loss, val_loss, train_acc, val_acc, colors,\n                       loss_legend_loc='upper center', acc_legend_loc='upper left',\n                       fig_size=(20, 10), sub_plot1=(1, 2, 1), sub_plot2=(1, 2, 2)):\n\n    plt.rcParams[\"figure.figsize\"] = fig_size\n    fig = plt.figure()\n\n    plt.subplot(sub_plot1[0], sub_plot1[1], sub_plot1[2])\n\n    for i in range(len(train_loss)):\n        x_train = range(len(train_loss[i]))\n        x_val = range(len(val_loss[i]))\n\n        min_train_loss = train_loss[i].min()\n\n        min_val_loss = val_loss[i].min()\n\n        plt.plot(x_train, train_loss[i], linestyle='-', color='tab:{}'.format(colors[i]),\n                 label=\"TRAIN LOSS ({0:.4})\".format(min_train_loss))\n        plt.plot(x_val, val_loss[i], linestyle='--' , color='tab:{}'.format(colors[i]),\n                 label=\"VALID LOSS ({0:.4})\".format(min_val_loss))\n\n    plt.xlabel('epoch no.')\n    plt.ylabel('loss')\n    plt.legend(loc=loss_legend_loc)\n    plt.title('Training and Validation Loss')\n\n    plt.subplot(sub_plot2[0], sub_plot2[1], sub_plot2[2])\n\n    for i in range(len(train_acc)):\n        x_train = range(len(train_acc[i]))\n        x_val = range(len(val_acc[i]))\n\n        max_train_acc = train_acc[i].max()\n\n        max_val_acc = val_acc[i].max()\n\n        plt.plot(x_train, train_acc[i], linestyle='-', color='tab:{}'.format(colors[i]),\n                 label=\"TRAIN ACC ({0:.4})\".format(max_train_acc))\n        plt.plot(x_val, val_acc[i], linestyle='--' , color='tab:{}'.format(colors[i]),\n                 label=\"VALID ACC ({0:.4})\".format(max_val_acc))\n\n    plt.xlabel('epoch no.')\n    plt.ylabel('accuracy')\n    plt.legend(loc=acc_legend_loc)\n    plt.title('Training and Validation Accuracy')\n\n    fig.savefig('sample_loss_acc_plot.png')\n    plt.show()\n\n    return","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">4. Train and Validation [5 Points]</font>\n\n\n**Write the methods or classes to be used for training and validation.**","metadata":{}},{"cell_type":"code","source":"def train(\n    train_config: TrainingConfiguration, model: nn.Module, optimizer: torch.optim.Optimizer,\n    train_loader: torch.utils.data.DataLoader, epoch_idx: int, tb_writer: SummaryWriter\n) -> None:\n\n    # change model in training mood\n    model.train()\n\n    # to get batch loss\n    batch_loss = np.array([])\n\n    # to get batch accuracy\n    batch_acc = np.array([])\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n\n        # clone target\n        indx_target = target.clone()\n        # send data to device (its is medatory if GPU has to be used)\n        data = data.to(train_config.device)\n        # send target to device\n        target = target.to(train_config.device)\n\n        # reset parameters gradient to zero\n        optimizer.zero_grad()\n\n        # forward pass to the model\n        output = model(data)\n\n        # cross entropy loss\n        loss = F.cross_entropy(output, target)\n\n        # find gradients w.r.t training parameters\n        loss.backward()\n        # Update parameters using gardients\n        optimizer.step()\n\n        batch_loss = np.append(batch_loss, [loss.item()])\n\n        # Score to probability using softmax\n        prob = F.softmax(output, dim=1)\n\n        # get the index of the max probability\n        pred = prob.data.max(dim=1)[1]\n\n        # correct prediction\n        correct = pred.cpu().eq(indx_target).sum()\n\n        # accuracy\n        acc = float(correct) / float(len(data))\n\n        batch_acc = np.append(batch_acc, [acc])\n\n        if batch_idx % train_config.log_interval == 0 and batch_idx > 0:\n\n            total_batch = epoch_idx * len(train_loader.dataset)/train_config.batch_size + batch_idx\n            tb_writer.add_scalar('Loss/train-batch', loss.item(), total_batch)\n            tb_writer.add_scalar('Accuracy/train-batch', acc, total_batch)\n\n\n    epoch_loss = batch_loss.mean()\n    epoch_acc = batch_acc.mean()\n    print('Epoch: {} \\nTrain Loss: {:.6f} Acc: {:.4f}'.format(epoch_idx, epoch_loss, epoch_acc))\n    return epoch_loss, epoch_acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate(\n    train_config: TrainingConfiguration,\n    model: nn.Module,\n    test_loader: torch.utils.data.DataLoader,\n) -> float:\n    #\n    model.eval()\n    test_loss = 0\n    count_corect_predictions = 0\n    for data, target in test_loader:\n        indx_target = target.clone()\n        data = data.to(train_config.device)\n\n        target = target.to(train_config.device)\n\n        output = model(data)\n        # add loss for each mini batch\n        test_loss += F.cross_entropy(output, target).item()\n\n        # Score to probability using softmax\n        prob = F.softmax(output, dim=1)\n\n        # get the index of the max probability\n        pred = prob.data.max(dim=1)[1]\n\n        # add correct prediction count\n        count_corect_predictions += pred.cpu().eq(indx_target).sum()\n\n    # average over number of mini-batches\n    test_loss = test_loss / len(test_loader)\n\n    # average over number of dataset\n    accuracy = 100. * count_corect_predictions / len(test_loader.dataset)\n\n    print(\n        '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n            test_loss, count_corect_predictions, len(test_loader.dataset), accuracy\n        )\n    )\n\n    return test_loss, accuracy/100.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main(model, optimizer, tb_writer, scheduler=None, system_configuration=SystemConfiguration(),\n         training_configuration=TrainingConfiguration(), data_augmentation=True):\n\n    # system configuration\n    setup_system(system_configuration)\n\n    # batch size\n    batch_size_to_set = training_configuration.batch_size\n    # num_workers\n    num_workers_to_set = training_configuration.num_workers\n    # epochs\n    epoch_num_to_set = training_configuration.epochs_count\n\n    # if GPU is available use training config,\n    # else lowers batch_size, num_workers and epochs count\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    else:\n        device = \"cpu\"\n        batch_size_to_set = 16\n        num_workers_to_set = 4\n\n    # data loader\n    train_loader, test_loader = get_data(train_data, val_loader,\n        batch_size=batch_size_to_set,\n        num_workers=num_workers_to_set,\n        data_augmentation=data_augmentation\n    )\n\n    # Update training configuration\n    training_configuration = TrainingConfiguration(\n        device=device,\n        batch_size=batch_size_to_set,\n        num_workers=num_workers_to_set\n    )\n\n    # send model to device (GPU/CPU)\n    model.to(training_configuration.device)\n\n    best_loss = torch.tensor(np.inf)\n\n    # epoch train/test loss\n    epoch_train_loss = np.array([])\n    epoch_test_loss = np.array([])\n\n    # epch train/test accuracy\n    epoch_train_acc = np.array([])\n    epoch_test_acc = np.array([])\n\n    # trainig time measurement\n    t_begin = time.time()\n    for epoch in range(training_configuration.epochs_count):\n\n#         Calculate Initial Test Loss\n        init_val_loss, init_val_accuracy = validate(training_configuration, model, test_loader)\n        print(\"Initial Test Loss : {:.6f}, \\nInitial Test Accuracy : {:.3f}%\\n\".format(init_val_loss, init_val_accuracy*100))\n\n        # Train\n        train_loss, train_acc = train(training_configuration, model, optimizer, train_loader, epoch, tb_writer)\n\n        epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n\n        epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n\n        # add scalar (loss/accuracy) to tensorboard\n        tb_writer.add_scalar('Loss/Train',train_loss, epoch)\n        tb_writer.add_scalar('Accuracy/Train', train_acc, epoch)\n\n        elapsed_time = time.time() - t_begin\n        speed_epoch = elapsed_time / (epoch + 1)\n        speed_batch = speed_epoch / len(train_loader)\n        eta = speed_epoch * training_configuration.epochs_count - elapsed_time\n\n        print(\n            \"Elapsed {:.2f}s, {:.2f} s/epoch, {:.2f} s/batch, ets {:.2f}s\".format(\n                elapsed_time, speed_epoch, speed_batch, eta\n            )\n        )\n\n        # add time metadata to tensorboard\n        tb_writer.add_scalar('Time/elapsed_time', elapsed_time, epoch)\n        tb_writer.add_scalar('Time/speed_epoch', speed_epoch, epoch)\n        tb_writer.add_scalar('Time/speed_batch', speed_batch, epoch)\n        tb_writer.add_scalar('Time/eta', eta, epoch)\n\n        # Validate\n        if epoch % training_configuration.test_interval == 0:\n            current_loss, current_accuracy = validate(training_configuration, model, test_loader)\n\n            epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n\n            epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n\n            # add scalar (loss/accuracy) to tensorboard\n            tb_writer.add_scalar('Loss/Validation', current_loss, epoch)\n            tb_writer.add_scalar('Accuracy/Validation', current_accuracy, epoch)\n\n            # add scalars (loss/accuracy) to tensorboard\n            tb_writer.add_scalars('Loss/train-val', {'train': train_loss,\n                                           'validation': current_loss}, epoch)\n            tb_writer.add_scalars('Accuracy/train-val', {'train': train_acc,\n                                               'validation': current_accuracy}, epoch)\n\n            if current_loss < best_loss:\n                best_loss = current_loss\n                print('Model Improved. Saving the Model...\\n')\n                save_model(model, device=training_configuration.device)\n\n\n        if scheduler is not None:\n            scheduler.step()\n\n\n    print(\"Total time: {:.2f}, Best Loss: {:.3f}\".format(time.time() - t_begin, best_loss))\n\n    return model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">5. Model [5 Points]</font>\n\n**Define your model in this section.**\n\n**You are allowed to use any pre-trained model.**","metadata":{}},{"cell_type":"code","source":"def save_model(model, device, model_dir='models_resnext', model_file_name='kenyan_food_classifier.pt'):\n\n\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n\n    model_path = os.path.join(model_dir, model_file_name)\n\n    # make sure you transfer the model to cpu.\n    if device == 'cuda':\n        model.to('cpu')\n\n    # save the state_dict\n    torch.save(model.state_dict(), model_path)\n\n    if device == 'cuda':\n        model.to('cuda')\n\n    return","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_model(model, model_dir='models_resnext', model_file_name='kenyan_food_classifier.pt'):\n    model_path = os.path.join(model_dir, model_file_name)\n\n    # loading the model and getting model parameters by using load_state_dict\n    model.load_state_dict(torch.load(model_path))\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pretrained_resnext50(num_class=13):\n    resnext = models.resnext50_32x4d(pretrained=True)\n\n    count=0\n    for child in resnext.children():\n        count+=1\n        if count<=7:\n            for param in child.parameters():\n                param.requires_grad = False\n\n\n\n    last_layer_in = resnext.fc.in_features\n    resnext.fc = nn.Linear(last_layer_in, num_class)\n\n\n    return resnext","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">6. Utils [5 Points]</font>\n\n**Define those methods or classes, which have  not been covered in the above sections.**","metadata":{}},{"cell_type":"code","source":"def plot_loss_accuracy(train_loss, val_loss, train_acc, val_acc, colors,\n                       loss_legend_loc='upper center', acc_legend_loc='upper left',\n                       fig_size=(20, 10), sub_plot1=(1, 2, 1), sub_plot2=(1, 2, 2)):\n\n    plt.rcParams[\"figure.figsize\"] = fig_size\n    fig = plt.figure()\n\n    plt.subplot(sub_plot1[0], sub_plot1[1], sub_plot1[2])\n\n    for i in range(len(train_loss)):\n        x_train = range(len(train_loss[i]))\n        x_val = range(len(val_loss[i]))\n\n        min_train_loss = train_loss[i].min()\n\n        min_val_loss = val_loss[i].min()\n\n        plt.plot(x_train, train_loss[i], linestyle='-', color='tab:{}'.format(colors[i]),\n                 label=\"TRAIN LOSS ({0:.4})\".format(min_train_loss))\n        plt.plot(x_val, val_loss[i], linestyle='--' , color='tab:{}'.format(colors[i]),\n                 label=\"VALID LOSS ({0:.4})\".format(min_val_loss))\n\n    plt.xlabel('epoch no.')\n    plt.ylabel('loss')\n    plt.legend(loc=loss_legend_loc)\n    plt.title('Training and Validation Loss')\n\n    plt.subplot(sub_plot2[0], sub_plot2[1], sub_plot2[2])\n\n    for i in range(len(train_acc)):\n        x_train = range(len(train_acc[i]))\n        x_val = range(len(val_acc[i]))\n\n        max_train_acc = train_acc[i].max()\n\n        max_val_acc = val_acc[i].max()\n\n        plt.plot(x_train, train_acc[i], linestyle='-', color='tab:{}'.format(colors[i]),\n                 label=\"TRAIN ACC ({0:.4})\".format(max_train_acc))\n        plt.plot(x_val, val_acc[i], linestyle='--' , color='tab:{}'.format(colors[i]),\n                 label=\"VALID ACC ({0:.4})\".format(max_val_acc))\n\n    plt.xlabel('epoch no.')\n    plt.ylabel('accuracy')\n    plt.legend(loc=acc_legend_loc)\n    plt.title('Training and Validation Accuracy')\n\n    fig.savefig('sample_loss_acc_plot.png')\n    plt.show()\n\n    return","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">7. Experiment [5 Points]</font>\n\n**Choose your optimizer and LR-scheduler and use the above methods and classes to train your model.**","metadata":{}},{"cell_type":"code","source":"model = pretrained_resnext50()\nprint(model)\n\n# get optimizer\ntrain_config = TrainingConfiguration()\n\n### CHANGE HERE ###\n\n# optimizer\noptimizer = optim.Adam(\n    model.parameters(),\n    lr = train_config.init_learning_rate, weight_decay = 0.01\n)\n\ndecay_rate = 0.01\n\nlmbda = lambda epoch: 1/(1 + decay_rate * epoch)\n\n# Scheduler\nscheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lmbda)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fine_tuning_sw = SummaryWriter('logs_kenyan_food_resnext')\n\nmodel, train_loss, train_acc, val_loss, val_acc = main(model, optimizer, fine_tuning_sw, scheduler=scheduler, data_augmentation=True)\n\nfine_tuning_sw.close()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">8. TensorBoard Dev Scalars Log Link [5 Points]</font>\n\n**Share your TensorBoard scalars logs link here You can also share (not mandatory) your GitHub link, if you have pushed this project in GitHub.**\n\n\nFor example, [Find Project2 logs here](https://tensorboard.dev/experiment/kMJ4YU0wSNG0IkjrluQ5Dg/#scalars).","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## <font style=\"color:green\">9. Kaggle Profile Link [50 Points]</font>\n\n**Share your Kaggle profile link  with us here to score , points in  the competition.**\n\n**For full points, you need a minimum accuracy of `75%` on the test data. If accuracy is less than `70%`, you gain  no points for this section.**\n\n\n**Submit `submission.csv` (prediction for images in `test.csv`), in the `Submit Predictions` tab in Kaggle, to get evaluated for  this section.**","metadata":{}}]}